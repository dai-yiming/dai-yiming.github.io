
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural Networks Guide</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "document", "processHtmlClass": "math|output_area"}}</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tools" href="../tools/index.html" />
    <link rel="prev" title="MATLAB for Finance" href="matlab_finance.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class=" collapse navbar-collapse">
    <div id="navbar-center" class="ml-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="index.html">
  Docs
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../tools/index.html">
  Tools
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/dai-yiming" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://www.linkedin.com/in/yimingdai/" rel="noopener" target="_blank" title="LinkedIn">
            <span><i class="fab fa-linkedin"></i></span>
            <label class="sr-only">LinkedIn</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs .." aria-label="Search the docs .." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="numpy_guide.html">
   Numpy Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="numpy_exercise.html">
   Numpy Exercises
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pandas_guide.html">
   pandas Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="scipy_guide.html">
   SciPy Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="matlab_guide.html">
   MATLAB Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="matlab_finance.html">
   MATLAB for Finance
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neural Networks Guide
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setting-up">
   Setting up
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#notations">
   Notations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logistic-regression">
   Logistic regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#definition">
     Definition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cost-function">
     Cost function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#derivatives">
     Derivatives
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent">
   Gradient descent
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Definition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-checking">
     Gradient checking
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mini-batch">
     Mini-batch
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#random-initialization">
   Random initialization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-network-basics">
   Neural network basics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forward-pass">
     Forward Pass
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dimensions">
     Dimensions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#backward-prop">
     Backward Prop
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activation-functions">
   Activation functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bias-variance">
   Bias &amp; Variance
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization">
   Regularization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     Logistic regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-network">
     Neural network
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weight-update">
     Weight update
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dropout-regularization">
     Dropout regularization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normalize-inputs">
     Normalize inputs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-methods">
     Other methods
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="neural-networks-guide">
<span id="id1"></span><h1>Neural Networks Guide<a class="headerlink" href="#neural-networks-guide" title="Permalink to this headline">¶</a></h1>
<p>This is an elegant-design introduction to Standard Deep Neural Networks (DNN), geared mainly for new users. A neural network can be illustrated as a computation graph with forward passes and backward propagation of biases. The performance of deep learning depends on several factors: data availability, GPU and computational power, algorithm like activation functions and optimizers, ML iterative cycle, size of DNN, etc. Vectorization is a fundamental tool to accelerate computation.</p>
<div class="section" id="setting-up">
<h2>Setting up<a class="headerlink" href="#setting-up" title="Permalink to this headline">¶</a></h2>
<p>Applied machine learning is a highly iterative process with recurrent components: idea, code implementation, and experiment. In modern era, the dataset is usually splitted into a ratio of 98/1/1 with training set being the largest component. There are four components that usually exist in a ML project:</p>
<ul class="simple">
<li><p>training set: build the</p></li>
<li><p>dev set (or hold-out cross validation set): optimize hyperparameters</p></li>
<li><p>test set: evaluate performance</p></li>
</ul>
<p>Make sure the data in the dev set and test set are from the same distribution.</p>
</div>
<div class="section" id="notations">
<h2>Notations<a class="headerlink" href="#notations" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(m\)</span> : number of examples</p></li>
<li><p><span class="math notranslate nohighlight">\(n_{x}\)</span> : input size (or single example size)</p></li>
<li><p><span class="math notranslate nohighlight">\(n_{y}\)</span> : output size (or number of classes)</p></li>
<li><p><span class="math notranslate nohighlight">\(L\)</span> : number of layers</p></li>
<li><p><span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n_{x} \times m}\)</span> : input matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(Y \in \mathbb{R}^{n_{y} \times m}\)</span> : label matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(x^{[l](i)}\)</span> : the <span class="math notranslate nohighlight">\(i^{th}\)</span> training example of the <span class="math notranslate nohighlight">\(l^{th}\)</span> layer</p></li>
<li><p><span class="math notranslate nohighlight">\(x_{h}\)</span> : the <span class="math notranslate nohighlight">\(h^{th}\)</span> hidden units</p></li>
<li><p><span class="math notranslate nohighlight">\(W^{[l]}\)</span> : weight matrix for <span class="math notranslate nohighlight">\(l^{th}\)</span> layer</p></li>
<li><p><span class="math notranslate nohighlight">\(b^{[l]}\)</span> : bias vector for <span class="math notranslate nohighlight">\(l^{th}\)</span> layer</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}\)</span> : predicted output vector (or <span class="math notranslate nohighlight">\(a^{[L]}\)</span>)</p></li>
</ul>
</div>
<div class="section" id="logistic-regression">
<h2>Logistic regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h2>
<div class="section" id="definition">
<h3>Definition<a class="headerlink" href="#definition" title="Permalink to this headline">¶</a></h3>
<p>Given parameters <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{n_x}\)</span>, <span class="math notranslate nohighlight">\(w \in \mathbb{R}^{n_x}\)</span>, and <span class="math notranslate nohighlight">\(b \in \mathbb{R}\)</span>, we want <span class="math notranslate nohighlight">\(\hat{y}\)</span>, with sigmoid function as the activation function, to be</p>
<div class="math notranslate nohighlight">
\[\hat{y} = P(y = 1 | x) = \sigma(z) = \sigma(w^{T}x + b)\]</div>
<p>where <span class="math notranslate nohighlight">\(0 \leq \hat{y} \leq 1\)</span>.</p>
<p>For the definition of the chosen activation function, refer to <a class="reference internal" href="#activation-functions"><span class="std std-ref">Activation functions</span></a></p>
</div>
<div class="section" id="cost-function">
<h3>Cost function<a class="headerlink" href="#cost-function" title="Permalink to this headline">¶</a></h3>
<p>Given <span class="math notranslate nohighlight">\(\left\{\left(x^{1}, y^{1}\right),\left(x^{2}, y^{2}\right), \ldots\left(x^{m}, y^{m}\right)\right\}\)</span>, we want <span class="math notranslate nohighlight">\(\hat{y}^{(i)} \approx y^{(i)}\)</span>. To calculate the cost function, we first need to choose a loss function which is a convex function</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\hat{y}, y) = - \left(y \log \hat{y} + (1 - y) \log (1 - \hat{y}) \right)\]</div>
<p>Commonly, if replacing <span class="math notranslate nohighlight">\(\hat{y}\)</span> with <span class="math notranslate nohighlight">\(a\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(a, y) = - \left(y \log a + (1 - y) \log (1 - a) \right)\]</div>
<p>The cost function is defined as</p>
<div class="math notranslate nohighlight">
\[\mathcal{J}(w, b)=\frac{1}{m} \sum_{i=1}^{m} \mathcal{L}\left(\hat{y}^{(i)}, y^{(i)}\right)=\frac{1}{m} \sum \mathcal{L}\left(a, y\right)\]</div>
</div>
<div class="section" id="derivatives">
<h3>Derivatives<a class="headerlink" href="#derivatives" title="Permalink to this headline">¶</a></h3>
<p>To calculate derivatives with respect to a certain variable in the cost function, use one step backward propagation on a computational graph with chain rule, calculating from right to left. Recall that <span class="math notranslate nohighlight">\(\hat{y} = a = \sigma(z)\)</span> where <span class="math notranslate nohighlight">\(z = w^{T}x + b)\)</span>, the solution to the derivative is</p>
<div class="math notranslate nohighlight">
\[dz = \frac{dL}{dz} = \frac{dL}{da} \frac{da}{dz} = \frac{a-y}{a(1-a)} \times a(1-a) = a - y = \hat{y} - y\]</div>
</div>
</div>
<div class="section" id="gradient-descent">
<h2>Gradient descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id2">
<h3>Definition<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>Gradient descent wants to find <span class="math notranslate nohighlight">\(w, b\)</span> that minimize <span class="math notranslate nohighlight">\(\mathcal{J}(w, b)\)</span>, i.e. to find the minima in the graph of convex loss function</p>
<div class="math notranslate nohighlight">
\[w = w - \alpha \frac{\partial \mathcal{J}}{\partial w}, \quad
b = b - \alpha \frac{\partial \mathcal{J}}{\partial b}\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate, and</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T, \quad
\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m \left(a^{(i)}-y^{(i)}\right)\]</div>
<p>For simplicity, we denote <span class="math notranslate nohighlight">\(dx\)</span> as the (partial) derivative of a variable, e.g. <span class="math notranslate nohighlight">\(dw = \frac{\partial J}{\partial w}\)</span>.</p>
<p>In a deep neural network, the activation and gradients will explode if <span class="math notranslate nohighlight">\(W &gt; I\)</span> where <span class="math notranslate nohighlight">\(I\)</span> is identity matrix; the gradients will vanish if <span class="math notranslate nohighlight">\(W &lt; I\)</span>. To solve this, refer to <a class="reference internal" href="#random-initialization"><span class="std std-ref">Random initialization</span></a>.</p>
</div>
<div class="section" id="gradient-checking">
<h3>Gradient checking<a class="headerlink" href="#gradient-checking" title="Permalink to this headline">¶</a></h3>
<p>Recall the definition of a derivative (or gradient) as</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}\]</div>
<p>Gradient checking tells you if your implementation of backpropagation is correct. We prefer to run gradient checking at random initialization and train the network for several iteration, and bugs may be detected from abnormal growth of weight and bias matrix.</p>
<ul class="simple">
<li><p>use it only for debugging purpose.</p></li>
<li><p>use backward propagation regularization version if <span class="math notranslate nohighlight">\(L_2\)</span> or <span class="math notranslate nohighlight">\(L_1\)</span> regularization is applied.</p></li>
<li><p>gradient checking doesn’t work with dropout regularization because the cost function is not consistent.</p></li>
<li><p>if algorithm fails grad check, look at components to try to identify bug.</p></li>
</ul>
<p>We implement by</p>
<ul class="simple">
<li><p>concatenate all flattened weight and bias matrices into a vector <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
<li><p>concatenate all flattened weight and bias derivative matrices into a vector <span class="math notranslate nohighlight">\(d\theta\)</span></p></li>
<li><p>calculate approximate gradient <span class="math notranslate nohighlight">\(d\theta\)</span> by</p></li>
</ul>
<div class="math notranslate nohighlight">
\[d \theta_{a p p r o x}^{[i]}=\frac{J\left(\theta_{1}, \theta_{2}, \ldots, \theta_{i}+\varepsilon, \ldots\right)-J\left(\theta_{1}, \theta_{2}, \ldots, \theta_{i}-\varepsilon, \ldots\right)}{2 \varepsilon} \approx d \theta^{[i]}=\frac{\partial J}{\partial \theta_{i}}\]</div>
<ul class="simple">
<li><p>calculate distance using normalized Euclidean distance as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\frac{\left\|d \theta_{a p p r o x}^{[i]}-d \theta\right\|_{2}}{\left\|d \theta_{a p p r o x}^{[i]}\right\|_{2}+\|d \theta\|_{2}}\]</div>
<p>Now, we interpret results as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{distance} \approx
\begin{cases}
             1e-7, &amp; \text{good implementation}\\
   1e-5, &amp; \text{further inspection}\\
   1e-3, &amp; \text{bad implementation}
\end{cases}\end{split}\]</div>
</div>
<div class="section" id="mini-batch">
<h3>Mini-batch<a class="headerlink" href="#mini-batch" title="Permalink to this headline">¶</a></h3>
<p>For large <span class="math notranslate nohighlight">\(m\)</span>, vectorization can still be slow. A solution is to use mini-batch gradient descent rather than batch gradient descent using explicit loop. We should choose a batch size between 1 and <span class="math notranslate nohighlight">\(m\)</span> since</p>
<ul class="simple">
<li><p>if batch size is 1, each example is a batch</p></li>
<li><p>if batch size is <span class="math notranslate nohighlight">\(m\)</span>, batch gradient descent is performed</p></li>
</ul>
<p>By convention, we choose a batch size of <span class="math notranslate nohighlight">\(2^n\)</span> with <span class="math notranslate nohighlight">\(n\)</span> determined by the size of the dataset. We do not consider stochastic gradient descent since it is too noisy, never converge, and breaking vectorization.</p>
<p>Recall that the graph of the cost funciton demonstrate a mostly monotonically decreasing function. For small batch size, the cost may volatile, but it will decrease macroscopically.</p>
</div>
</div>
<div class="section" id="random-initialization">
<span id="id3"></span><h2>Random initialization<a class="headerlink" href="#random-initialization" title="Permalink to this headline">¶</a></h2>
<p>Initializing the bias to zero is acceptable; but initializing the weights to zero causes different neurons learn with identical output since they are symmetrical. A common practice is to randomly assign small weights with a defined statistical distribution; if the weights are large, some activation functions tend to saturate in large values, i.e. slow learning rate. Thus, the weights should be initialized randomly to break symmetry</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layerDim</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layerDim</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layerDim</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<p>To avoid gradient vanishing or exploding, we want to achieve a variance of value <span class="math notranslate nohighlight">\(\frac{2}{n}\)</span> by using He initialization with ReLU activations, randomly initializing weights around zero but either above and below equally</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layerDim</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layerDim</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">layerDim</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p>Other recommended activation function options are Xavier or Bengio et al activation</p>
<div class="math notranslate nohighlight">
\[tanh\left(\sqrt{\frac{1}{n^{[l-1]}}}\right) \quad \text{or} \quad
tanh\left(\sqrt{\frac{2}{n^{[l]} + n^{[l-1]}}}\right)\]</div>
</div>
<div class="section" id="neural-network-basics">
<h2>Neural network basics<a class="headerlink" href="#neural-network-basics" title="Permalink to this headline">¶</a></h2>
<p>Neurons in the shallower layers learn simple features of data, while neurons in the deeper layers learn complex charadcteristics. Usually, we count the sum of the number of hidden layers and the output layer as the total layers for a neural network, i.e. we do not count input layer.</p>
<p>Each neuron is a two-step process computing <span class="math notranslate nohighlight">\(z\)</span> and the activation function of that, and each layer has its own chosen activation function with dimension correspondent parameters.</p>
<p>The general methodology to build a Neural Network is to</p>
<ol class="arabic simple">
<li><p>define the neural network structure, initialize the model’s parameters, and define hyperparameters</p></li>
<li><p>loop:</p>
<ul class="simple">
<li><p>implement forward propagation and generate cache</p></li>
<li><p>compute loss</p></li>
<li><p>implement backward propagation with cache to get the gradients</p></li>
<li><p>update parameters with gradient descent</p></li>
</ul>
</li>
<li><p>use trained parameters to predict labels</p></li>
</ol>
<div class="section" id="forward-pass">
<h3>Forward Pass<a class="headerlink" href="#forward-pass" title="Permalink to this headline">¶</a></h3>
<p>Given parameters <span class="math notranslate nohighlight">\(A^{[l-1]}\)</span>, the formulae of forward propagation in a DNN are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
   Z^{[l]} &amp; = W^{[l]} A^{[l-1]} + b^{[l]}\\
   A^{[l]} &amp; = g^{[l]}\left(Z^{[l]}\right)
\end{align*}\end{split}\]</div>
<p>generating <span class="math notranslate nohighlight">\(A^{[l]}\)</span> and cache. In both ends we have</p>
<div class="math notranslate nohighlight">
\[A^{0} = X \text{ and } A^{L} = \hat{y}\]</div>
</div>
<div class="section" id="dimensions">
<h3>Dimensions<a class="headerlink" href="#dimensions" title="Permalink to this headline">¶</a></h3>
<p>The dimensions for a <span class="math notranslate nohighlight">\(m\)</span> examples are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
   W^{[l]} &amp; \quad \left(n^{[l]}, n^{[l-1]}\right)\\
   dW^{[l]} &amp; \quad \left(n^{[l]}, n^{[l-1]}\right)\\
   b^{[l]} &amp; \quad \left(n^{[l]}, m\right)\\
   db^{[l]} &amp; \quad \left(n^{[l]}, m\right)\\
   Z^{[l]} &amp; \quad \left(n^{[l]}, m\right)\\
   dZ^{[l]} &amp; \quad \left(n^{[l]}, m\right)\\
   A^{[l]} &amp; \quad \left(n^{[l]}, m\right)\\
   dA^{[l]} &amp; \quad \left(n^{[l]}, m\right)\\
\end{align*}\end{split}\]</div>
</div>
<div class="section" id="backward-prop">
<h3>Backward Prop<a class="headerlink" href="#backward-prop" title="Permalink to this headline">¶</a></h3>
<p>Given parameters <span class="math notranslate nohighlight">\(dA^{[l]}\)</span> and caches from forward propagation, the formulae of backward propagation in a DNN are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
   dZ^{[l]} &amp; = \frac{\partial \mathcal{J} }{\partial Z^{[l]}} = dA^{[l]} * g'(Z^{[l]})\\
   dW^{[l]} &amp; = \frac{\partial \mathcal{J} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]}A^{[l-1]T}\\
   db^{[l]} &amp; = \frac{\partial \mathcal{J} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} \left(dZ^{[l](i)}\right)\\
   dZ^{[l-1]} &amp; = \frac{\partial \mathcal{J} }{\partial Z^{[l-1]}} = dW^{[l]T} dZ^{[l]} g^{\prime [l]}\left(Z^{[l-1]}\right)\\
   dA^{[l-1]} &amp; = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}\\
\end{align*}\end{split}\]</div>
<p>generating <span class="math notranslate nohighlight">\(dA^{[l-1]}\)</span>, <span class="math notranslate nohighlight">\(dW^{[l]}\)</span>, and <span class="math notranslate nohighlight">\(db^{[l]}\)</span>. If we choose sigmoid activation function, we have</p>
<div class="math notranslate nohighlight">
\[dZ^{[l]} = \frac{\partial \mathcal{J} }{\partial Z^{[l]}} = A^{[l]} - Y\]</div>
</div>
</div>
<div class="section" id="activation-functions">
<span id="id4"></span><h2>Activation functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">¶</a></h2>
<p>Linear activation function resembles identity function, equipping with less solving power to the neurons. Thus, linear activation functions are commonly used in the output layer.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function is defined as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\sigma(z) = \frac{1}{1 + e^{-z}} \approx
\begin{cases}
             1, &amp; \text{if } z \to \infty\\
   0.5, &amp; \text{if } z = 0\\
   0, &amp; \text{if } z \to \infty
\end{cases}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\sigma^{\prime}(z) = a(1-a) \approx
\begin{cases}
             0, &amp; \text{if } z \to \infty\\
   0.25, &amp; \text{if } z = 0\\
   0, &amp; \text{if } z \to \infty
\end{cases}\end{split}\]</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tanh</span></code> function is mostly better than sigmoid function since it pushes teh mean to zero to make next learning process easier</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} \approx
\begin{cases}
              1, &amp; \text{if } z \to \infty\\
    0, &amp; \text{if } z = 0\\
   -1, &amp; \text{if } z \to \infty
\end{cases}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}tanh^{\prime}(z) \approx
\begin{cases}
             0, &amp; \text{if } z \to \infty\\
   1, &amp; \text{if } z = 0\\
   0, &amp; \text{if } z \to \infty
\end{cases}\end{split}\]</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ReLU</span></code> function (Rectified Linear Unit) learns much faster because of constant slope</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}ReLU(z) = max(0, z) \approx
\begin{cases}
             z, &amp; \text{if } z \geq 0\\
   0, &amp; \text{if } z &lt; 0
\end{cases}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}ReLU^{\prime}(z) \approx
\begin{cases}
             1, &amp; \text{if } z \geq 0\\
   0, &amp; \text{if } z &lt; 0
\end{cases}\end{split}\]</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">leaky</span> <span class="pre">ReLU</span></code> function (Rectified Linear Unit) is usually defined with <span class="math notranslate nohighlight">\(\alpha = 0.01\)</span> as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}leaky\_ReLU(z) = max(0, \alpha z) \approx
\begin{cases}
             z, &amp; \text{if } z \geq 0\\
   \alpha z, &amp; \text{if } z &lt; 0
\end{cases}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}leaky\_ReLU^{\prime}(z) \approx
\begin{cases}
             1, &amp; \text{if } z \geq 0\\
   \alpha , &amp; \text{if } z &lt; 0
\end{cases}\end{split}\]</div>
</div>
<div class="section" id="bias-variance">
<h2>Bias &amp; Variance<a class="headerlink" href="#bias-variance" title="Permalink to this headline">¶</a></h2>
<p>The basic assumption is that the human orror or optimal base error is 0%. If a model is underfitting, high bias occurs. Try</p>
<ul class="simple">
<li><p>to make your NN bigger (size of hidden units, number of layers)</p></li>
<li><p>to run it longer</p></li>
<li><p>a different model that is suitable for your data</p></li>
<li><p>a different (advanced) optimization algorithm</p></li>
</ul>
<p>If a model is overfitting, high variance occurs. Try</p>
<ul class="simple">
<li><p>more data</p></li>
<li><p>regularization</p></li>
<li><p>a different model that is suitable for your data</p></li>
</ul>
<p>until your algorithm has a low bias and a low variance.</p>
</div>
<div class="section" id="regularization">
<h2>Regularization<a class="headerlink" href="#regularization" title="Permalink to this headline">¶</a></h2>
<p>We often encounter two regularization schemes</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L_2\)</span> regularization is <span class="math notranslate nohighlight">\(\|m\|^{2}_{2} = \sum^{n_x}_{i = 1} w^2_i = w^Tw\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(L_1\)</span> regularization is <span class="math notranslate nohighlight">\(\|m\|_{1} = \sum^{n_x}_{i = 1} |w_i|\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(L_2\)</span> regularization also named weight decay, and <span class="math notranslate nohighlight">\(L_1\)</span> regularization shrinks model size by changing some weights to zero.</p></li>
</ul>
<div class="section" id="id5">
<h3>Logistic regression<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>Regularization for logistic regression is to minimize <span class="math notranslate nohighlight">\(\mathcal{J}(w,b)\)</span> with regularization parameter <span class="math notranslate nohighlight">\(\lambda\)</span>, which is a hyperparameter in the DNN</p>
<div class="math notranslate nohighlight">
\[J_{regularized} = \small \underbrace{ \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}\left(\hat{y}^{(i)}, y^{(i)}\right) }_\text{cross-entropy cost} + \small \underbrace{ \frac{\lambda}{2 m} \|w\|_{2}^{2} + \frac{\lambda}{2 m} \sum_{i=1}^{m} b^{(i)^2} }_{L_2 \text{ regularization cost}}\]</div>
<p>where the last term, or regularization on bias, is usually omitted. The denominator of the regularization term is a scale parameter.</p>
</div>
<div class="section" id="neural-network">
<h3>Neural network<a class="headerlink" href="#neural-network" title="Permalink to this headline">¶</a></h3>
<p>Regularization for neural networks is to minimize <span class="math notranslate nohighlight">\(\mathcal{J}\)</span> for all layers, that is</p>
<div class="math notranslate nohighlight">
\[J\left(w^{[1]}, b^{[1]}, \ldots, w^{[L]}, b^{[L]}\right)=\frac{1}{m} \sum_{i=1}^{n} \mathcal{L}\left(\hat{y}^{(i)}, y^{(i)}\right)+\frac{\lambda}{2 m} \sum_{l=1}^{L}\left\|w^{[l]}\right\|_{F}^{2}\]</div>
<p>where <span class="math notranslate nohighlight">\(\|w^{[l]}\|^2_F = \sum^{n^{[l]}}_{i = 1} \sum^{n^{[l-1]}}_{j=1} \left(w_{ij}^{[l]}\right)\)</span> is the Frobenius norm.</p>
</div>
<div class="section" id="weight-update">
<h3>Weight update<a class="headerlink" href="#weight-update" title="Permalink to this headline">¶</a></h3>
<p>Recall from backward propagation we have</p>
<div class="math notranslate nohighlight">
\[dW^{[l]} = \frac{\partial \mathcal{J} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]}A^{[l-1]T}\]</div>
<p>With regularization, we update by</p>
<div class="math notranslate nohighlight">
\[dW^{[l]} = \frac{1}{m} dZ^{[l]}A^{[l-1]T} + \frac{\lambda}{m} W^{[l]}\]</div>
<p>then weight matrix update process becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
   W^{[l]} &amp; = W^{[l]} - \alpha dW^{[l]}\\
   &amp; = W^{[l]} - \alpha \left[\frac{1}{m} dZ^{[l]}A^{[l-1]T} + \frac{\lambda}{m} W^{[l]} \right]\\
   &amp; = W^{[l]} - \frac{\alpha \lambda}{m} W^{[l]} - \alpha \left[\frac{1}{m} dZ^{[l]}A^{[l-1]T} \right]\\
   &amp; = \left(1 - \frac{\alpha \lambda}{m}\right) - \alpha \left[\frac{1}{m} dZ^{[l]}A^{[l-1]T} \right]\\
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\left(1 - \frac{\alpha \lambda}{m}\right) &lt; 1\)</span> causing the weight decays, with an inverse relationship exists between parameters <span class="math notranslate nohighlight">\(W^{[l]}\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(\lambda\)</span> is set too large, some weights become zero causing a smaller neural network. Take <code class="docutils literal notranslate"><span class="pre">tanh</span></code> activation function, if <span class="math notranslate nohighlight">\(\lambda\)</span> is properly set, the activations are roughly linear, learning fastest and preventing overfitting.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When implementing gradient descent, one debug method is to plot the cost function <span class="math notranslate nohighlight">\(\mathcal{J}\)</span> as a function of the number of iterations of gradient descent.</p>
<p>The cost function should decrease monotonically after every evaluation of gradient descent with regularization. If you plot the cost function without regularization, then you might not see it decrease monotonically.</p>
</div>
</div>
<div class="section" id="dropout-regularization">
<h3>Dropout regularization<a class="headerlink" href="#dropout-regularization" title="Permalink to this headline">¶</a></h3>
<p>The intuition is that the model cannot rely on any features, so have to spread out weights. Formally, dropout regularization eliminates some neurons/weights on each iteration based on a probability. A most common technique is inverted dropout</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># set 0 &lt;= keep_prob &lt;= 1</span>
<span class="n">keep_prob</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="c1"># initialize matrix d</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">a</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="c1"># convert entries of d to 0 or 1</span>
<span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="n">d</span> <span class="o">&lt;</span> <span class="n">keep_prob</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="c1"># shut down some neurons of a</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">d</span>
<span class="c1"># scale the value of alive neurons</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="n">keep_prob</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Dropout can have different value of parameter <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code> per layer.</p></li>
<li><p>The dropout for input layer has to be near one.</p></li>
<li><p>Dropout isn’t used at test time because it would add noise to predictions.</p></li>
<li><p>Apply dropout both during forward and backward propagation.</p></li>
<li><p>If you’re more worried about some layers overfitting than others, apply dropout to certain layers with just one hyperparameter <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code>.</p></li>
<li><p>A downside of dropout is that the cost function is not well defined, hard to debug. To solve this, turn off dropout, i.e. setting <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code> to one, and check the cost function do decrease monotonically.</p></li>
</ul>
</div>
<div class="section" id="normalize-inputs">
<h3>Normalize inputs<a class="headerlink" href="#normalize-inputs" title="Permalink to this headline">¶</a></h3>
<p>Normalize inputs will speed up the training process. We use the mean and variance of teh training set to apply normalization to training, dev, and test sets in the way</p>
<div class="math notranslate nohighlight">
\[\frac{x^{(i)} - \mu}{\sigma^2}\]</div>
<p>The cost function is inconsistent (or elongated) and hard to optimize. With normalization, the cost function will be consistent and symmetrical and optimized faster with less volatility.</p>
</div>
<div class="section" id="other-methods">
<h3>Other methods<a class="headerlink" href="#other-methods" title="Permalink to this headline">¶</a></h3>
<p><strong>Data augmentation</strong> is to create more data based on current data easily and cheaply, e.g. image rotation, horizontally flipping, zooming.</p>
<p><strong>Early stopping</strong> may be of cheaper price and shorter time to prevent overfitting. Usually, we plot the training set and the dev set cost together for each iteration. At some iteration the dev set cost will stop decreasing and will start increasing, and we pick the point at which the training set error and dev set error are best.</p>
<ul class="simple">
<li><p>One advantage is that no hyperparameters are investigated, unlike lambda in <span class="math notranslate nohighlight">\(L_2\)</span> regularization.</p></li>
<li><p>One downside is that early stopping tries to simultaneously optimize the cost function with gradient descent and prevent overfitting with regularization, breaking orthogonalization. Hence, we prefer to use <span class="math notranslate nohighlight">\(L_2\)</span> regularization</p></li>
</ul>
<p><strong>Under construction !! Expected publish date to be Aug 15, 2021.</strong></p>
</div>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="matlab_finance.html" title="previous page">MATLAB for Finance</a>
    <a class='right-next' id="next-link" href="../tools/index.html" title="next page">Tools</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Yiming Dai.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>