
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural Networks Guide</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "document", "processHtmlClass": "math|output_area"}}</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tools" href="../tools/index.html" />
    <link rel="prev" title="MATLAB for Finance" href="matlab_finance.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class=" collapse navbar-collapse">
    <div id="navbar-center" class="ml-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="index.html">
  Docs
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../tools/index.html">
  Tools
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/dai-yiming" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://www.linkedin.com/in/yimingdai/" rel="noopener" target="_blank" title="LinkedIn">
            <span><i class="fab fa-linkedin"></i></span>
            <label class="sr-only">LinkedIn</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs .." aria-label="Search the docs .." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="numpy_guide.html">
   Numpy Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="numpy_exercise.html">
   Numpy Exercises
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pandas_guide.html">
   pandas Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="scipy_guide.html">
   SciPy Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="matlab_guide.html">
   MATLAB Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="matlab_finance.html">
   MATLAB for Finance
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neural Networks Guide
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#notations">
   Notations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logistic-regression">
   Logistic regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#definition">
     Definition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cost-function">
     Cost function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     Gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#derivatives">
     Derivatives
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#random-initialization">
   Random initialization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-network-basics">
   Neural network basics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forward-pass">
     Forward Pass
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dimensions">
     Dimensions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#backward-prop">
     Backward Prop
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activation-functions">
   Activation functions
  </a>
 </li>
</ul>

</nav>
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="neural-networks-guide">
<span id="id1"></span><h1>Neural Networks Guide<a class="headerlink" href="#neural-networks-guide" title="Permalink to this headline">¶</a></h1>
<p>This is an elegant-design introduction to Standard Deep Neural Networks (DNN), geared mainly for new users. A neural network can be illustrated as a computation graph with forward passes and backward propagation of biases. The performance of deep learning depends on several factors: data availability, GPU and computational power, algorithm like activation functions and optimizers, ML iterative cycle, size of DNN, etc. Vectorization is a fundamental tool to accelerate computation.</p>
<div class="section" id="notations">
<h2>Notations<a class="headerlink" href="#notations" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(m\)</span> : number of examples</p></li>
<li><p><span class="math notranslate nohighlight">\(n_{x}\)</span> : input size (or single example size)</p></li>
<li><p><span class="math notranslate nohighlight">\(n_{y}\)</span> : output size (or number of classes)</p></li>
<li><p><span class="math notranslate nohighlight">\(L\)</span> : number of layers</p></li>
<li><p><span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n_{x} \times m}\)</span> : input matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(Y \in \mathbb{R}^{n_{y} \times m}\)</span> : label matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(x^{[l](i)}\)</span> : the <span class="math notranslate nohighlight">\(i^{th}\)</span> training example of the <span class="math notranslate nohighlight">\(l^{th}\)</span> layer</p></li>
<li><p><span class="math notranslate nohighlight">\(x_{h}\)</span> : the <span class="math notranslate nohighlight">\(h^{th}\)</span> hidden units</p></li>
<li><p><span class="math notranslate nohighlight">\(W^{[l]}\)</span> : weight matrix for <span class="math notranslate nohighlight">\(l^{th}\)</span> layer</p></li>
<li><p><span class="math notranslate nohighlight">\(b^{[l]}\)</span> : bias vector for <span class="math notranslate nohighlight">\(l^{th}\)</span> layer</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}\)</span> : predicted output vector (or <span class="math notranslate nohighlight">\(a^{[L]}\)</span>)</p></li>
</ul>
</div>
<div class="section" id="logistic-regression">
<h2>Logistic regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h2>
<div class="section" id="definition">
<h3>Definition<a class="headerlink" href="#definition" title="Permalink to this headline">¶</a></h3>
<p>Given parameters <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{n_x}\)</span>, <span class="math notranslate nohighlight">\(w \in \mathbb{R}^{n_x}\)</span>, and <span class="math notranslate nohighlight">\(b \in \mathbb{R}\)</span>, we want <span class="math notranslate nohighlight">\(\hat{y}\)</span>, with sigmoid function as the activation function, to be</p>
<div class="math notranslate nohighlight">
\[\hat{y} = P(y = 1 | x) = \sigma(z) = \sigma(w^{T}x + b)\]</div>
<p>where <span class="math notranslate nohighlight">\(0 \leq \hat{y} \leq 1\)</span>.</p>
<p>For the definition of the chosen activation function, refer to <a class="reference internal" href="#activation-functions"><span class="std std-ref">Activation functions</span></a></p>
</div>
<div class="section" id="cost-function">
<h3>Cost function<a class="headerlink" href="#cost-function" title="Permalink to this headline">¶</a></h3>
<p>Given <span class="math notranslate nohighlight">\(\left\{\left(x^{1}, y^{1}\right),\left(x^{2}, y^{2}\right), \ldots\left(x^{m}, y^{m}\right)\right\}\)</span>, we want <span class="math notranslate nohighlight">\(\hat{y}^{(i)} \approx y^{(i)}\)</span>. To calculate the cost function, we first need to choose a loss function which is a convex function</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\hat{y}, y) = - \left(y \log \hat{y} + (1 - y) \log (1 - \hat{y}) \right)\]</div>
<p>Commonly, if we replace <span class="math notranslate nohighlight">\(\hat{y}\)</span> with <span class="math notranslate nohighlight">\(a\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(a, y) = - \left(y \log a + (1 - y) \log (1 - a) \right)\]</div>
<p>The cost function is defined as</p>
<div class="math notranslate nohighlight">
\[\mathcal{J}(w, b)=\frac{1}{m} \sum_{i=1}^{m} \mathcal{L}\left(\hat{y}^{(i)}, y^{(i)}\right)=\frac{1}{m} \sum \mathcal{L}\left(a, y\right)\]</div>
</div>
<div class="section" id="gradient-descent">
<h3>Gradient descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h3>
<p>Gradient descent wants to find <span class="math notranslate nohighlight">\(w, b\)</span> that minimize <span class="math notranslate nohighlight">\(\mathcal{J}(w, b)\)</span>, i.e. to find the minima in the graph of convex loss function</p>
<div class="math notranslate nohighlight">
\[w = w - \alpha \frac{\partial \mathcal{J}}{\partial w}, \quad
b = b - \alpha \frac{\partial \mathcal{J}}{\partial b}\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate, and</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T, \quad
\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m \left(a^{(i)}-y^{(i)}\right)\]</div>
<p>For simplicity, we denote <span class="math notranslate nohighlight">\(dx\)</span> as the (partial) derivative of a variable, e.g. <span class="math notranslate nohighlight">\(dw = \frac{\partial J}{\partial w}\)</span>.</p>
</div>
<div class="section" id="derivatives">
<h3>Derivatives<a class="headerlink" href="#derivatives" title="Permalink to this headline">¶</a></h3>
<p>To calculate derivatives with respect to a certain variable in the cost function, use one step backward propagation on a computational graph with chain rule, calculating from right to left. Recall that <span class="math notranslate nohighlight">\(\hat{y} = a = \sigma(z)\)</span> where <span class="math notranslate nohighlight">\(z = w^{T}x + b)\)</span>, the solution to the derivative is</p>
<div class="math notranslate nohighlight">
\[dz = \frac{dL}{dz} = \frac{dL}{da} \frac{da}{dz} = \frac{a-y}{a(1-a)} \times a(1-a) = a - y = \hat{y} - y\]</div>
</div>
</div>
<div class="section" id="random-initialization">
<h2>Random initialization<a class="headerlink" href="#random-initialization" title="Permalink to this headline">¶</a></h2>
<p>Initializing the bias to zero is acceptable; but initializing the weights to zero causes different neurons learn with identical output since they are symmetrical. A common practice is to randomly assign small weights with a defined statistical distribution; if the weights are large, some activation functions tend to saturate in large values, i.e. slow learning rate.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="neural-network-basics">
<h2>Neural network basics<a class="headerlink" href="#neural-network-basics" title="Permalink to this headline">¶</a></h2>
<p>Neurons in the shallower layers learn simple features of data, while neurons in the deeper layers learn complex charadcteristics. Usually, we count the sum of the number of hidden layers and the output layer as the total layers for a neural network, i.e. we do not count input layer.</p>
<p>Each neuron is a two-step process computing <span class="math notranslate nohighlight">\(z\)</span> and the activation function of that, and each layer has its own chosen activation function with dimension correspondent parameters.</p>
<p>The general methodology to build a Neural Network is to</p>
<ol class="arabic simple">
<li><p>Define the neural network structure, initialize the model’s parameters, and define hyperparameters</p></li>
<li><p>Loop:</p>
<ul class="simple">
<li><p>Implement forward propagation and generate cache</p></li>
<li><p>Compute loss</p></li>
<li><p>Implement backward propagation with cache to get the gradients</p></li>
<li><p>Update parameters with gradient descent</p></li>
</ul>
</li>
<li><p>Use trained parameters to predict labels</p></li>
</ol>
<div class="section" id="forward-pass">
<h3>Forward Pass<a class="headerlink" href="#forward-pass" title="Permalink to this headline">¶</a></h3>
<p>The formulae of forward propagation in a DNN are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
   Z^{[l]} &amp; = W^{[l]} A^{[l-1]} + b^{[l]}\\
   A^{[l]} &amp; = g^{[l]}\left(Z^{[l]}\right)
\end{align*}\end{split}\]</div>
<p>where in both ends we have</p>
<div class="math notranslate nohighlight">
\[A^{0} = X \text{ and } A^{L} = \hat{y}\]</div>
</div>
<div class="section" id="dimensions">
<h3>Dimensions<a class="headerlink" href="#dimensions" title="Permalink to this headline">¶</a></h3>
<p>The dimensions for a <span class="math notranslate nohighlight">\(m\)</span> examples are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
   W^{[l]} &amp; \quad \left(n^{[l]}, n^{[l-1]}\right)\\
   dW^{[l]} &amp; \quad \left(n^{[l]}, n^{[l-1]}\right)\\
   b^{[l]} &amp; \quad \left(n^{[l]}, m\right)\\
   db^{[l]} &amp; \quad \left(n^{[l]}, m\right)\\
   Z^{[l]} &amp; \quad \left(n^{[l]}, m\right)\\
   dZ^{[l]} &amp; \quad \left(n^{[l]}, m\right)\\
   A^{[l]} &amp; \quad \left(n^{[l]}, m\right)\\
   dA^{[l]} &amp; \quad \left(n^{[l]}, m\right)\\
\end{align*}\end{split}\]</div>
</div>
<div class="section" id="backward-prop">
<h3>Backward Prop<a class="headerlink" href="#backward-prop" title="Permalink to this headline">¶</a></h3>
<p>The formulae of backward propagation in a DNN are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
   dZ^{[l]} &amp; = \frac{\partial \mathcal{J} }{\partial Z^{[l]}} = A^{[l]} - Y\\
   dW^{[l]} &amp; = \frac{\partial \mathcal{J} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]}A^{[l-1]T}\\
   db^{[l]} &amp; = \frac{\partial \mathcal{J} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} \left(dZ^{[l](i)}\right)\\
   dZ^{[l-1]} &amp; = \frac{\partial \mathcal{J} }{\partial Z^{[l-1]}} = dW^{[l]T} dZ^{[l]} g^{\prime [l]}\left(Z^{[l-1]}\right)
\end{align*}\end{split}\]</div>
</div>
</div>
<div class="section" id="activation-functions">
<span id="id2"></span><h2>Activation functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">¶</a></h2>
<p>Linear activation function resembles identity function, equipping with less solving power to the neurons. Thus, linear activation functions are commonly used in the output layer.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function is defined as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\sigma(z) = \frac{1}{1 + e^{-z}} \approx
\begin{cases}
             1, &amp; \text{if } z \to \infty\\
   0.5, &amp; \text{if } z = 0\\
   0, &amp; \text{if } z \to \infty
\end{cases}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\sigma^{\prime}(z) = a(1-a) \approx
\begin{cases}
             0, &amp; \text{if } z \to \infty\\
   0.25, &amp; \text{if } z = 0\\
   0, &amp; \text{if } z \to \infty
\end{cases}\end{split}\]</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tanh</span></code> function is mostly better than sigmoid function since it pushes teh mean to zero to make next learning process easier</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} \approx
\begin{cases}
              1, &amp; \text{if } z \to \infty\\
    0, &amp; \text{if } z = 0\\
   -1, &amp; \text{if } z \to \infty
\end{cases}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}tanh^{\prime}(z) \approx
\begin{cases}
             0, &amp; \text{if } z \to \infty\\
   1, &amp; \text{if } z = 0\\
   0, &amp; \text{if } z \to \infty
\end{cases}\end{split}\]</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ReLU</span></code> function (Rectified Linear Unit) learns much faster because of constant slope</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}ReLU(z) = max(0, z) \approx
\begin{cases}
             z, &amp; \text{if } z \geq 0\\
   0, &amp; \text{if } z &lt; 0
\end{cases}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}ReLU^{\prime}(z) \approx
\begin{cases}
             1, &amp; \text{if } z \geq 0\\
   0, &amp; \text{if } z &lt; 0
\end{cases}\end{split}\]</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">leaky</span> <span class="pre">ReLU</span></code> function (Rectified Linear Unit) is usually defined with <span class="math notranslate nohighlight">\(\alpha = 0.01\)</span> as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}leaky\_ReLU(z) = max(0, \alpha z) \approx
\begin{cases}
             z, &amp; \text{if } z \geq 0\\
   \alpha z, &amp; \text{if } z &lt; 0
\end{cases}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}leaky\_ReLU^{\prime}(z) \approx
\begin{cases}
             1, &amp; \text{if } z \geq 0\\
   \alpha , &amp; \text{if } z &lt; 0
\end{cases}\end{split}\]</div>
<p><strong>Under construction !! Expected publish date to be Aug 15, 2021.</strong></p>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="matlab_finance.html" title="previous page">MATLAB for Finance</a>
    <a class='right-next' id="next-link" href="../tools/index.html" title="next page">Tools</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Yiming Dai.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>